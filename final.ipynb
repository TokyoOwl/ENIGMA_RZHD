{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N-9gfES54UI",
        "outputId": "d0355911-e1a1-4415-9c8c-c9390f0740bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename ='/content/drive/MyDrive/output.csv'\n",
        "\n",
        "reader = pd.read_csv(filename)\n",
        "question = reader['question']\n",
        "questions = []\n",
        "\n",
        "for i in question:\n",
        "  questions.append(i)"
      ],
      "metadata": {
        "id": "uXOuJ-v4576D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers_merged = []\n",
        "answer2 = reader['answers_merged']\n",
        "for j in answer2:\n",
        "  answers_merged.append(j)\n",
        "ans_m = []\n",
        "for i in range(len(answers_merged)):\n",
        "  an = answers_merged[i][2:len(answers_merged[0]) - 2]\n",
        "  an = an.replace(']', '')\n",
        "  an = an.replace('\\\\n', '')\n",
        "  an = an.replace(\"'\", \"\").split('.,')\n",
        "  ans_m.append(an)"
      ],
      "metadata": {
        "id": "y4hKpqtd6C8j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diction = {\n",
        " 'ЖД' : 'железная дорога',\n",
        " 'КВЗ-5' : 'тележка грузовых и пассажирских вагонов КВЗ-5',\n",
        " 'КВЗ-И2' : 'тележка грузовых и пассажирских вагонов КВЗ-И2',\n",
        " 'МВПС' : 'моторвагонный пассажирский состав',\n",
        " 'ПТЭ' : 'Правила технической эксплуатации',\n",
        " 'РФ' : 'Российская Федерация',\n",
        " 'С_П' : 'Свод правил',\n",
        " 'СА-3' : 'Автосцепка СА-3',\n",
        " 'ССПС' : 'Специальный самоходный подвижной состав',\n",
        " 'ТР' : 'Ремонт тягового подвижного состава',\n",
        " 'ТС' : 'Технологическая служба'}"
      ],
      "metadata": {
        "id": "cMSFxqDM6Ee6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "answer1 = reader['answer_summary']\n",
        "for j in answer1:\n",
        "  answers.append(j)"
      ],
      "metadata": {
        "id": "1oeFdeyS6JOx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {}\n",
        "for i in range(len(answers)):\n",
        "    dic[question[i]] = answers[i]"
      ],
      "metadata": {
        "id": "e1i4T-Ly6L96"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "sXRHFygs6nhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "model_checkpoint = 'cointegrated/rubert-base-cased-nli-threeway'"
      ],
      "metadata": {
        "id": "XXlC83N86gxr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic(text1, text2):\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "  if torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "  with torch.inference_mode():\n",
        "      out = model(**tokenizer (text1, text2, return_tensors='pt').to(model.device))\n",
        "      proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n",
        "\n",
        "  fin_dict = {v: proba[k] for k, v in model.config.id2label.items()}\n",
        "  l1 = max(fin_dict, key=fin_dict.get)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "      out = model(**tokenizer (text2, text1, return_tensors='pt').to(model.device))\n",
        "      proba = torch.softmax(out.logits, -1).cpu().numpy()[0]\n",
        "\n",
        "  fin_dict = {v: proba[k] for k, v in model.config.id2label.items()}\n",
        "\n",
        "  l2 = max(fin_dict, key=fin_dict.get)\n",
        "  if l2 == 'entailment':\n",
        "    return l2\n",
        "  else:\n",
        "    return l1\n"
      ],
      "metadata": {
        "id": "nDdvaDLM6iV6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import spacy\n",
        "from spacy.lang.ru.examples import sentences\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"ru_core_news_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhDkoLdiUUcL",
        "outputId": "4de8aabe-a699-4531-cbc5-0644092b7a74"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'ru_core_news_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.6.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  directory, to prevent mismatches with relative paths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vern = 0\n",
        "vopr = 0\n",
        "keys = list(dic.keys())\n",
        "n = random.choice(keys)\n",
        "while True:\n",
        "  vopr += 1\n",
        "  print(n)\n",
        "  inp = input()\n",
        "  doc1 = nlp(dic[n])\n",
        "  doc2 = nlp(inp)\n",
        "  if n == inp:\n",
        "    pass\n",
        "  elif doc1.similarity(doc2) > 0.15:\n",
        "    if semantic(inp, dic[n]) == 'entailment':\n",
        "      print('Верно')\n",
        "      print('')\n",
        "      vern += 1\n",
        "      n = random.choice(keys)\n",
        "    else:\n",
        "      print('Ответ неправильный')\n",
        "      print('')\n",
        "      print(dic[n])\n",
        "      print('')\n",
        "      n = random.choice(keys)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "82Mj6aLp6zN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "b19d58b9-9431-4be2-e2f1-b7c7a900ea7a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Каким документом утверждаются технология, порядок обслуживания и технические параметры содержания элементов подвижного состава и инфраструктуры?\n",
            "как поставить чай\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-a8231a869597>:13: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  elif doc1.similarity(doc2) > 0.15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Каким документом утверждаются технология, порядок обслуживания и технические параметры содержания элементов подвижного состава и инфраструктуры?\n",
            "я поставил чай и отошел поесть далеко далеко\n",
            "Каким документом утверждаются технология, порядок обслуживания и технические параметры содержания элементов подвижного состава и инфраструктуры?\n",
            "Каким документом утверждаются технология, порядок обслуживания и технические параметры содержания элементов подвижного состава и инфраструктуры?\n",
            "Каким документом утверждаются технология, порядок обслуживания и технические параметры содержания элементов подвижного состава и инфраструктуры?\n",
            "локальным нормативным актом владельца инфраструктуры\n",
            "Верно\n",
            "\n",
            "Как следует трактовать сигнал одного жёлтого огня на светофоре и какое состояние следующего светофора это обозначает в контексте движения поезда?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-a8231a869597>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mvopr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvs4EOw6XG0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}